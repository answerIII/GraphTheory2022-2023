{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Importing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "datasets = [\n",
    "    \"data/opsahl-ucsocial/out.opsahl-ucsocial\",\n",
    "    \"data/emails/email-Eu-core-temporal_processed.txt\",\n",
    "    \"data/soc-sign-bitcoinalpha/out.soc-sign-bitcoinalpha\",\n",
    "    \"data/soc-sign-bitcoinotc/out.soc-sign-bitcoinotc\",\n",
    "    \"data/munmun_digg_reply/out.munmun_digg_reply\",\n",
    "    \"data/sx-mathoverflow/out.sx-mathoverflow\"\n",
    "]\n",
    "skiprows = [[0, 1], [], [0], [0], [0], [0]]\n",
    "current = 0 # 0 1 2 3 4 5\n",
    "graph = pd.read_csv(\n",
    "    datasets[current],\n",
    "    names=[\"_from\", \"_to\", \"_weight\", \"_timestamp\"],\n",
    "    sep=\" |\\t\",\n",
    "    engine ='python',\n",
    "    skiprows=skiprows[current]\n",
    ")\n",
    "# print(graph.dtypes)\n",
    "# print(graph.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helpful tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(number_of_task: int, lines: list[str]):\n",
    "    \"\"\"Writes results to .txt files.\n",
    "    Args:\n",
    "        number_of_task (int): 1 or 2.\n",
    "        lines (list[str]): Lines to write.\n",
    "    \"\"\"\n",
    "    with open(f\"results/task_{number_of_task}/dataset_{current}.txt\", \"w\") as f:\n",
    "        f.writelines(line + '\\n' for line in lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1899, volume = 59835\n"
     ]
    }
   ],
   "source": [
    "V = np.unique(graph[\"_from\"]._append(graph[\"_to\"])).astype(int)\n",
    "V.sort()\n",
    "n = V.size\n",
    "volume = graph[\"_timestamp\"].size\n",
    "print(f\"{n = }, {volume = }\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Chapter 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Preparing static graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "graph_static = dict.fromkeys(V)\n",
    "for u in V:\n",
    "    graph_static[u] = set()\n",
    "for _index, _from, _to, _weight, _timestamp in graph.itertuples():\n",
    "    if _from == _to:\n",
    "        continue\n",
    "    if not _to in V:\n",
    "        print(_from, _to)\n",
    "    graph_static[_from].add(_to)\n",
    "    graph_static[_to].add(_from)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "E_count = 0\n",
    "for u in graph_static.keys():\n",
    "    E_count += len(graph_static[u])\n",
    "E_count //= 2\n",
    "#print(E_count)\n",
    "\n",
    "density = E_count * 2 / (n * (n - 1))\n",
    "#print(density)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "V_to_visit = set(V)\n",
    "connectivity_components = []\n",
    "while(V_to_visit):\n",
    "    V_seen = set()\n",
    "    queue = []\n",
    "    for u in V_to_visit:\n",
    "        queue.append(u)\n",
    "        V_seen.add(u)\n",
    "        break\n",
    "    while queue:\n",
    "        u = queue.pop()\n",
    "        u_adjacent_to_visit = graph_static[u].difference(V_seen)\n",
    "        for v in u_adjacent_to_visit:\n",
    "            V_seen.add(v)\n",
    "            queue.append(v)\n",
    "    V_to_visit = V_to_visit.difference(V_seen)\n",
    "    connectivity_components.append(V_seen)\n",
    "\n",
    "sizes = list(map(lambda x: len(x), connectivity_components))\n",
    "max_component_size = max(sizes)\n",
    "max_connectivity_component_index = sizes.index(max_component_size)\n",
    "proportion = max_component_size / len(V)\n",
    "#print(f\"{max_val = }, {max_connectivity_component_index = }, {proportion = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 1.1\n",
    "print(\"|V| = %i, |E| = %i, p = %f, number of components = %i, max component size = %i, max component proportion = %f\"\n",
    "      % (n, E_count, density, len(connectivity_components), max_component_size, proportion))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1790778 = 1790778 True\n"
     ]
    }
   ],
   "source": [
    "component = list(connectivity_components[max_connectivity_component_index])\n",
    "distances = [0] * (n + 2)\n",
    "diameter = 0\n",
    "radius = n + 1\n",
    "n_limit = 10000\n",
    "if n < n_limit:\n",
    "    for start in component:\n",
    "        distances_tmp = dict.fromkeys(V, n + 1)\n",
    "        V_visited = set()\n",
    "        queue = [(start, 0)]\n",
    "        queued = set([start])\n",
    "        depth = 0\n",
    "        u = start\n",
    "        while queued:\n",
    "            (u, depth) = queue.pop(0)\n",
    "            queued.remove(u)\n",
    "            V_visited.add(u)\n",
    "            u_adjacent_to_visit = graph_static[u].difference(V_visited)\n",
    "            for v in u_adjacent_to_visit:\n",
    "                distances_tmp[v] = min(distances_tmp[v], depth + 1)\n",
    "                if v not in queued:\n",
    "                    queue.append((v, depth + 1))\n",
    "                    queued.add(v)\n",
    "        for u in V:\n",
    "            if u > start:\n",
    "                distances[distances_tmp[u]] += 1\n",
    "        diameter = max(diameter, depth)\n",
    "        radius = min(radius, depth)\n",
    "        if not start % 10:\n",
    "            #print(start, \"/\", n)\n",
    "            pass\n",
    "all_dist = 0\n",
    "for i in range(diameter + 1):\n",
    "    all_dist += distances[i]\n",
    "print(all_dist, \"=\", max_component_size * (max_component_size - 1) // 2,\n",
    "      all_dist == max_component_size * (max_component_size - 1) // 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#percentile_90 = np.percentile(all_distances, 90)\n",
    "percentile_90_ind = int(0.9 * all_dist)\n",
    "percentile_90 = 0\n",
    "ind_tmp = 0\n",
    "for i in range(diameter + 1):\n",
    "    ind_tmp += distances[i]\n",
    "    if ind_tmp >= percentile_90_ind:\n",
    "        percentile_90 = i\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_matrix(vertices):\n",
    "\n",
    "    vertices_list = list(vertices)\n",
    "\n",
    "    distances = []\n",
    "    distance_matrix = dict()\n",
    "    for u in vertices_list:\n",
    "        distance_matrix[u] = dict()\n",
    "        for v in vertices_list:\n",
    "            if u != v:\n",
    "                distance_matrix[u][v] = n + 1\n",
    "\n",
    "    for start in vertices_list:\n",
    "        V_to_calculate = set(vertices)\n",
    "        V_to_calculate.discard(start)\n",
    "        V_visited = set()\n",
    "        queue = [(start, 0)]\n",
    "        queued = set([start])\n",
    "        max_depth = 0\n",
    "        while queued and V_to_calculate:\n",
    "            u, depth = queue.pop(0)\n",
    "            max_depth = max(max_depth, depth)\n",
    "            queued.discard(u)\n",
    "            V_visited.add(u)\n",
    "            u_adjacent_to_visit = graph_static[u].difference(V_visited)\n",
    "            for v in u_adjacent_to_visit:\n",
    "                if v in V_to_calculate:\n",
    "                    distance = distance_matrix[start][v]\n",
    "                    if depth + 1 < distance:\n",
    "                        distance_matrix[start][v] = depth + 1\n",
    "                        distance_matrix[v][start] = depth + 1\n",
    "                        V_to_calculate.discard(v)\n",
    "                if v not in queued:\n",
    "                    queue.append((v, depth + 1))\n",
    "                    queued.add(v)\n",
    "    \n",
    "    for u in vertices_list:\n",
    "        for v in vertices_list:\n",
    "            if u > v:\n",
    "                distances.append(distance_matrix[u][v])\n",
    "    eccentricities = dict()\n",
    "    for u in distance_matrix:\n",
    "        eccentricities[u] = max(distance_matrix[u].values())\n",
    "\n",
    "    return (distance_matrix, eccentricities, distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diameter_from_random_500 = 8, radius_from_random_500 = 4, percentile_90_from_random_500 = 4.0\n",
      "diameter_from_random_1000 = 8, radius_from_random_1000 = 4, percentile_90_from_random_1000 = 4.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "component = list(connectivity_components[max_connectivity_component_index])\n",
    "diameter_from_random_500 = 0\n",
    "radius_from_random_500 = 0\n",
    "percentile_90_from_random_500 = 0\n",
    "diameter_from_random_1000 = 0\n",
    "radius_from_random_1000 = 0\n",
    "percentile_90_from_random_1000 = 0\n",
    "if n >= 500:\n",
    "    random_500_vertices = sorted(random.sample(component, 500))\n",
    "    random_500_matrix, random_500_eccentricities, random_500_distances = calculate_matrix(\n",
    "        random_500_vertices)\n",
    "    diameter_from_random_500 = max(random_500_eccentricities.values())\n",
    "    radius_from_random_500 = min(random_500_eccentricities.values())\n",
    "    percentile_90_from_random_500 = np.percentile(random_500_distances, 90)\n",
    "    print(f\"{diameter_from_random_500 = }, {radius_from_random_500 = }, {percentile_90_from_random_500 = }\")\n",
    "if n >= 1000:\n",
    "    random_1000_vertices = sorted(random.sample(component, 1000))\n",
    "    random_1000_matrix, random_1000_eccentricities, random_1000_distances = calculate_matrix(\n",
    "        random_1000_vertices)\n",
    "    diameter_from_random_1000 = max(random_1000_eccentricities.values())\n",
    "    radius_from_random_1000 = min(random_1000_eccentricities.values())\n",
    "    percentile_90_from_random_1000 = np.percentile(random_1000_distances, 90)\n",
    "    print(f\"{diameter_from_random_1000 = }, {radius_from_random_1000 = }, {percentile_90_from_random_1000 = }\")\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diameter_from_snowball_500 = 4, radius_from_snowball_500 = 3, percentile_90_from_snowball_500 = 3.0\n",
      "diameter_from_snowball_1000 = 5, radius_from_snowball_1000 = 3, percentile_90_from_snowball_1000 = 3.0\n"
     ]
    }
   ],
   "source": [
    "def snowball(limit):\n",
    "    vertices = {component[0], component[1]}\n",
    "    while len(vertices) < limit:\n",
    "        for v in vertices:\n",
    "            if len(vertices) < limit:\n",
    "                vertices = vertices.union(graph_static[v])\n",
    "    return sorted(list(vertices))\n",
    "\n",
    "\n",
    "diameter_from_snowball_500 = 0\n",
    "radius_from_snowball_500 = 0\n",
    "percentile_90_from_snowball_500 = 0\n",
    "diameter_from_snowball_1000 = 0\n",
    "radius_from_snowball_1000 = 0\n",
    "percentile_90_from_snowball_1000 = 0\n",
    "if n >= 500:\n",
    "    snowball_500_vertices = snowball(500)\n",
    "    snowball_500_matrix, snowball_500_eccentricities, snowball_500_distances = calculate_matrix(\n",
    "        snowball_500_vertices)\n",
    "    diameter_from_snowball_500 = max(snowball_500_eccentricities.values())\n",
    "    radius_from_snowball_500 = min(snowball_500_eccentricities.values())\n",
    "    percentile_90_from_snowball_500 = np.percentile(snowball_500_distances, 90)\n",
    "    print(f\"{diameter_from_snowball_500 = }, {radius_from_snowball_500 = }, {percentile_90_from_snowball_500 = }\")\n",
    "\n",
    "if n >= 1000:\n",
    "    snowball_1000_vertices = snowball(1000)\n",
    "    snowball_1000_matrix, snowball_1000_eccentricities, snowball_1000_distances = calculate_matrix(\n",
    "        snowball_1000_vertices)\n",
    "    diameter_from_snowball_1000 = max(snowball_1000_eccentricities.values())\n",
    "    radius_from_snowball_1000 = min(snowball_1000_eccentricities.values())\n",
    "    percentile_90_from_snowball_1000 = np.percentile(snowball_1000_distances, 90)\n",
    "    print(f\"{diameter_from_snowball_1000 = }, {radius_from_snowball_1000 = }, {percentile_90_from_snowball_1000 = }\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diameter = 8, raduis = 4, percentile_90 = 4\n"
     ]
    }
   ],
   "source": [
    "# 1.2\n",
    "print(\"diameter = %i, raduis = %i, percentile_90 = %i\" \n",
    "      % (diameter, radius, percentile_90))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "component = list(connectivity_components[max_connectivity_component_index])\n",
    "\n",
    "Cl = dict()\n",
    "for u in component:\n",
    "    u_neighbors = graph_static[u]\n",
    "\n",
    "    if len(u_neighbors) < 2:\n",
    "        Cl[u] = 0\n",
    "        continue\n",
    "\n",
    "    Lu_doubled = 0\n",
    "    for neighbor in u_neighbors:\n",
    "        Lu_doubled += len(graph_static[neighbor].intersection(u_neighbors))\n",
    "    Cl[u] = Lu_doubled / (len(u_neighbors) * (len(u_neighbors) - 1))\n",
    "\n",
    "Cl_average = sum(Cl.values()) / len(Cl.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#1.3\n",
    "print(\"Cl_average = %f\" % (Cl_average))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Task 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "R1 = 0\n",
    "R2 = 0\n",
    "R3 = 0\n",
    "Re = 0\n",
    "for u in V:\n",
    "    ku = len(graph_static[u])\n",
    "    R1 += ku\n",
    "    R2 += ku**2\n",
    "    R3 += ku**3\n",
    "    for v in graph_static[u]:\n",
    "        kv = len(graph_static[v])\n",
    "        Re += ku * kv\n",
    "degree_associativity = (Re * R1 - R2**2) / (R3 * R1 - R2**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Degree associativity = %f\" % (degree_associativity))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Chapter 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "t_min = graph[\"_timestamp\"].min()\n",
    "t_max = graph[\"_timestamp\"].max()\n",
    "s = 2.0 / 3.0\n",
    "t_s = t_min + (t_max - t_min) * s\n",
    "l = 0.2\n",
    "\n",
    "t_matrix = dict.fromkeys(V)\n",
    "for u in V:\n",
    "    t_matrix[u] = dict()\n",
    "\n",
    "for _index, _from, _to, _weight, _timestamp in graph.itertuples():\n",
    "    if _from == _to or _timestamp > t_s:\n",
    "        continue\n",
    "    \n",
    "    if t_matrix[_from].get(_to) is not None: \n",
    "        t_matrix[_from][_to].append(_timestamp)\n",
    "    else:\n",
    "        t_matrix[_from][_to] = [_timestamp]\n",
    "    \n",
    "    if t_matrix[_to].get(_from) is not None: \n",
    "        t_matrix[_to][_from].append(_timestamp)\n",
    "    else:\n",
    "        t_matrix[_to][_from] = [_timestamp]\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Static topological features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_static_topological_features(u, v):\n",
    "    gamma_u = set(t_matrix[u].keys())\n",
    "    gamma_v = set(t_matrix[v].keys())\n",
    "    \n",
    "    intersection_u_v = gamma_u.intersection(gamma_v)\n",
    "    \n",
    "    CN_static = len(intersection_u_v)\n",
    "    JC_static = CN_static / len(gamma_u.union(gamma_v))\n",
    "    PA_static = len(gamma_u) * len(gamma_v)\n",
    "    AA_static = sum(\n",
    "        [1.0 / np.log(len(set(t_matrix[z].keys()))) for z in intersection_u_v]\n",
    "    )\n",
    "    return CN_static, AA_static, JC_static, PA_static"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Node activity features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 1: Temporal weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_temporal_weighting(l, t):\n",
    "    time_var = (t - t_min) / (t_s - t_min)\n",
    "    w_linear = l + (1 - l) * time_var\n",
    "    w_exponential = l + (1 - l) * (np.exp(3 * time_var) - 1) / (np.exp(3) - 1)\n",
    "    w_square_root = l + (1 - l) * np.sqrt(time_var)\n",
    "    return [w_linear, w_exponential, w_square_root]\n",
    "\n",
    "for vt in t_matrix.values():\n",
    "    for v, times in vt.items():\n",
    "        for i, t in enumerate(times):\n",
    "            vt[v][i] = get_temporal_weighting(l, t)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 2: Aggregation of node activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class AggregationOfNodeActivity:  \n",
    "    @staticmethod\n",
    "    def zeroth_quantile(weights):\n",
    "        return np.quantile(weights, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def first_quantile(weights):\n",
    "        return np.quantile(weights, 0.25)\n",
    "    \n",
    "    @staticmethod\n",
    "    def second_quantile(weights):\n",
    "        return np.quantile(weights, 0.50)\n",
    "    \n",
    "    @staticmethod\n",
    "    def third_quantile(weights):\n",
    "        return np.quantile(weights, 0.75)\n",
    "    \n",
    "    @staticmethod\n",
    "    def fourth_quantile(weights):\n",
    "        return np.quantile(weights, 1)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_sum(weights):\n",
    "        return sum(weights)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_mean(weights):\n",
    "        return np.mean(weights)\n",
    "    \n",
    "    @staticmethod\n",
    "    def aggregate(weights):\n",
    "        return [\n",
    "            AggregationOfNodeActivity.zeroth_quantile(weights),\n",
    "            AggregationOfNodeActivity.first_quantile(weights),\n",
    "            AggregationOfNodeActivity.second_quantile(weights),\n",
    "            AggregationOfNodeActivity.third_quantile(weights),\n",
    "            AggregationOfNodeActivity.fourth_quantile(weights),\n",
    "            AggregationOfNodeActivity.get_sum(weights),\n",
    "            AggregationOfNodeActivity.get_mean(weights)\n",
    "        ]\n",
    "\n",
    "\n",
    "node_activity_feature_graph = dict()\n",
    "for u in V:\n",
    "    node_activity_feature_graph[u] = list()\n",
    "\n",
    "for u, vt in t_matrix.items():\n",
    "    if len(vt) == 0:\n",
    "        continue\n",
    "    linears = []\n",
    "    exponentials = []\n",
    "    squares = []\n",
    "    for v, weights in vt.items():\n",
    "        if not linears:\n",
    "            linears = [w[0] for w in weights]\n",
    "            exponentials = [w[1] for w in weights]\n",
    "            squares = [w[2] for w in weights]\n",
    "            continue\n",
    "        linears.extend([w[0] for w in weights])\n",
    "        exponentials.extend([w[1] for w in weights])\n",
    "        squares.extend([w[2] for w in weights])\n",
    "    for weights in (linears, exponentials, squares):\n",
    "        node_activity_feature_graph[u].extend(\n",
    "            AggregationOfNodeActivity.aggregate(weights)\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Step 3: Combining node activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CombiningNodeActivity:\n",
    "    @staticmethod\n",
    "    def get_sum(a, b):\n",
    "        return a + b\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_absolute_differrence(a, b):\n",
    "        return abs(a - b)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_minimum(a, b):\n",
    "        return min(a, b)\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_maximum(a, b):\n",
    "        return max(a, b)\n",
    "    \n",
    "    @staticmethod\n",
    "    def combine(u_list, v_list):\n",
    "        feature = []\n",
    "        for a, b in zip(u_list, v_list):\n",
    "            feature.append(CombiningNodeActivity.get_sum(a, b))\n",
    "            feature.append(CombiningNodeActivity.get_absolute_differrence(a, b))\n",
    "            feature.append(CombiningNodeActivity.get_minimum(a, b))\n",
    "            feature.append(CombiningNodeActivity.get_maximum(a, b))\n",
    "        return feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pairs = set()\n",
    "\n",
    "for u in V:\n",
    "    u_adj = set(t_matrix[u].keys())\n",
    "    for v in V:\n",
    "        if u >= v or v in u_adj:\n",
    "            continue\n",
    "        v_adj = set(t_matrix[v].keys())\n",
    "        if v_adj.intersection(u_adj):\n",
    "            pairs.add((u, v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220 334060\n"
     ]
    }
   ],
   "source": [
    "connecting = list()\n",
    "non_connecting = list()\n",
    "\n",
    "for u, v in pairs:\n",
    "    if v in graph_static[u]:\n",
    "        connecting.append((u, v))\n",
    "    else:\n",
    "        non_connecting.append((u, v))\n",
    "\n",
    "print(len(connecting), len(non_connecting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m = 10000\n",
    "x_y = random.choices(connecting, k=m)\n",
    "x_n = random.choices(non_connecting, k=m)\n",
    "\n",
    "# all = train + test\n",
    "\n",
    "x_pairs = list(x_y)\n",
    "x_pairs.extend(x_n)\n",
    "\n",
    "y_all = [1] * m\n",
    "y_all.extend([0] * m)\n",
    "\n",
    "features_temporal = dict.fromkeys(list(set(x_y).union(set(x_n))))\n",
    "features_static = dict.fromkeys(list(set(x_y).union(set(x_n))))\n",
    "\n",
    "for u, v in features_temporal.keys():\n",
    "    features_static[(u, v)] = get_static_topological_features(u, v)\n",
    "    features_temporal[(u, v)] = CombiningNodeActivity.combine(\n",
    "        node_activity_feature_graph[u], node_activity_feature_graph[v]\n",
    "    )\n",
    "\n",
    "x_all_static = []\n",
    "x_all_temporal = []\n",
    "\n",
    "for pair in x_pairs:\n",
    "    x_all_static.append(features_static[pair])\n",
    "    x_all_temporal.append(features_temporal[pair])\n",
    "\n",
    "\n",
    "y_train = []\n",
    "y_test = []\n",
    "x_train_static = []\n",
    "x_test_static = []\n",
    "x_train_temporal = []\n",
    "x_test_temporal = []\n",
    "x_pairs_train = []\n",
    "x_pairs_test = []\n",
    "\n",
    "for i in range(2 * m):\n",
    "    if random.random() > 0.75:\n",
    "        y_test.append(y_all[i])\n",
    "        x_test_static.append(x_all_static[i])\n",
    "        x_test_temporal.append(x_all_temporal[i])\n",
    "        x_pairs_test.append(x_pairs[i])\n",
    "    else:\n",
    "        y_train.append(y_all[i])\n",
    "        x_train_static.append(x_all_static[i])\n",
    "        x_train_temporal.append(x_all_temporal[i])\n",
    "        x_pairs_train.append(x_pairs[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8255020080321285\n"
     ]
    }
   ],
   "source": [
    "model_temporal = LogisticRegression(max_iter=10000)\n",
    "model_temporal.fit(x_train_temporal, y_train)\n",
    "y_pred_temporal = model_temporal.predict(x_test_temporal)\n",
    "print(metrics.accuracy_score(y_test, y_pred_temporal))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6853413654618474\n"
     ]
    }
   ],
   "source": [
    "model_static = LogisticRegression(max_iter=1000)\n",
    "model_static.fit(x_train_static, y_train)\n",
    "y_pred_static = model_static.predict(x_test_static)\n",
    "print(metrics.accuracy_score(y_test, y_pred_static))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Writing results down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [\n",
    "    \"Task 1.1\",\n",
    "    f\"Число вершин: |V| = {n}\",\n",
    "    f\"Число ребер: |E| = {E_count}\",\n",
    "    f\"Плотность: p = {density}\"\n",
    "    f\"Число компонент слабой связности: {len(connectivity_components)}\",\n",
    "    f\"Доля вершин в максимальной по мощности компоненте слабой связности: {max_component_size}\",\n",
    "    \"\\nTask 1.2\",\n",
    "    f\"Оценки для 500 вершин (из наибольшей КСС): {diameter_from_random_500 = }, {radius_from_random_500 = }, {percentile_90_from_random_500 = }\",\n",
    "    f\"Оценки для 1000 вершин (из наибольшей КСС): {diameter_from_random_1000 = }, {radius_from_random_1000 = }, {percentile_90_from_random_1000 = }\",\n",
    "    f\"Оценки для 500 вершин (снежный ком): {diameter_from_snowball_500 = }, {radius_from_snowball_500 = }, {percentile_90_from_snowball_500 = }\",\n",
    "    f\"Оценки для 1000 вершин (снежный ком): {diameter_from_snowball_1000 = }, {radius_from_snowball_1000 = }, {percentile_90_from_snowball_1000 = }\",\n",
    "    \"a\" if True: else \"b\",\n",
    "    \n",
    "]\n",
    "write_results(1, lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'E_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mn\u001b[39m \u001b[39m\u001b[39m= }\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mvolume\u001b[39m \u001b[39m\u001b[39m= }\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m|V| = \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m, |E| = \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m, p = \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m, number of components = \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m, max component size = \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m, max component proportion = \u001b[39m\u001b[39m%f\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m         \u001b[39m%\u001b[39m (n, E_count, density, \u001b[39mlen\u001b[39m(connectivity_components), max_component_size, proportion))\n\u001b[1;32m      7\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m f\u001b[39m.\u001b[39mwrite(\u001b[39m\"\u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m = \u001b[39m\u001b[39m%i\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m\n\u001b[1;32m      9\u001b[0m         (all_dist, max_component_size \u001b[39m*\u001b[39m (max_component_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m, all_dist \u001b[39m==\u001b[39m max_component_size \u001b[39m*\u001b[39m (max_component_size \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m \u001b[39m2\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'E_count' is not defined"
     ]
    }
   ],
   "source": [
    "f = open(\"results/Dataset\"+str(current)+\".txt\", \"w\")\n",
    "\n",
    "f.write(f\"{n = }, {volume = }\")\n",
    "f.write('\\n')\n",
    "f.write(\"|V| = %i, |E| = %i, p = %f, number of components = %i, max component size = %i, max component proportion = %f\"\n",
    "        % (n, E_count, density, len(connectivity_components), max_component_size, proportion))\n",
    "f.write('\\n')\n",
    "# f.write(\"%i = %i, %s\" %\n",
    "#         (all_dist, max_component_size * (max_component_size - 1) // 2, all_dist == max_component_size * (max_component_size - 1) // 2))\n",
    "# f.write('\\n')\n",
    "f.write(f\"{diameter_from_random_500 = }, {radius_from_random_500 = }, {percentile_90_from_random_500 = }\")\n",
    "f.write('\\n')\n",
    "f.write(f\"{diameter_from_random_1000 = }, {radius_from_random_1000 = }, {percentile_90_from_random_1000 = }\")\n",
    "f.write('\\n')\n",
    "f.write(f\"{diameter_from_snowball_500 = }, {radius_from_snowball_500 = }, {percentile_90_from_snowball_500 = }\")\n",
    "f.write('\\n')\n",
    "f.write(f\"{diameter_from_snowball_1000 = }, {radius_from_snowball_1000 = }, {percentile_90_from_snowball_1000 = }\")\n",
    "f.write('\\n')\n",
    "if n < n_limit:\n",
    "    f.write(\"diameter = %i, raduis = %i, percentile_90 = %i\" % (diameter, radius, percentile_90))\n",
    "else:\n",
    "    f.write(\"could not find diameter, raduis, percentile_90\")\n",
    "f.write('\\n')\n",
    "f.write(\"Cl_average = %f\" % (Cl_average))\n",
    "f.write('\\n')\n",
    "f.write(\"Degree associativity = %f\" % (degree_associativity))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
