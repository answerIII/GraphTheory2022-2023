{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт данных\n",
    "Для работы были выбраны следующие датасеты:\n",
    "\n",
    "1. Bitcoin Alpha: http://konect.cc/networks/soc-sign-bitcoinalpha/\n",
    "\n",
    "2. MathOverflow: http://konect.cc/networks/sx-mathoverflow/\n",
    "\n",
    "3. Chess: http://konect.cc/networks/chess/\n",
    "\n",
    "4. UC Irvine messages: http://konect.cc/networks/opsahl-ucsocial/\n",
    "\n",
    "5. Digg: http://konect.cc/networks/munmun_digg_reply/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from random import sample\n",
    "from random import choices\n",
    "from math import exp, sqrt\n",
    "import csv\n",
    "import json\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Инициализация графа средствами pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = 1\n",
    "# names = ['Bitcoin', 'Math', 'Chess', 'Ucsocial', 'Digg']\n",
    "\n",
    "# # choose dataset\n",
    "# datasets = ['soc-sign-bitcoinalpha', 'sx-mathoverflow', 'chess', 'opsahl-ucsocial', 'munmun_digg_reply']\n",
    "# filepath = filepath = \"./datasets/out.\" + datasets[dataset]\n",
    "\n",
    "# # define graph as dataframe structure\n",
    "# headers = [\"ID_of_from_node\",\"ID_of_to_node\", \"weight\", \"timestamp\"]\n",
    "# Graph = pd.read_csv(filepath, comment='%', names=headers, sep=' ', index_col=False).drop_duplicates()\n",
    "\n",
    "# # define number of vertices\n",
    "# V=np.unique((Graph['ID_of_from_node']._append(Graph['ID_of_to_node'])).values).astype(int)\n",
    "# V_num = V.size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тестового набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 9\n",
    "names = ['testgraph_1', 'testgraph_2', 'testgraph_3', 'testgraph_4', 'testgraph_5', 'testgraph_6', 'testgraph_7', 'socfb-Middlebury45', 'socfb-Reed98', 'team_10']\n",
    "\n",
    "# choose dataset\n",
    "datasets = ['testgraph_1', 'testgraph_2', 'testgraph_3', 'testgraph_4', 'testgraph_5', 'testgraph_6', 'testgraph_7', 'socfb-Middlebury45', 'socfb-Reed98', 'team_10']\n",
    "filepath = filepath = \"./test/\" + datasets[dataset] + \".txt\"\n",
    "\n",
    "# define graph as dataframe structure\n",
    "headers = [\"ID_of_from_node\",\"ID_of_to_node\", \"weight\", \"timestamp\"]\n",
    "Graph = pd.read_csv(filepath, comment='%', names=headers, sep=' ', index_col=False).drop_duplicates()\n",
    "\n",
    "# define number of vertices\n",
    "V=np.unique((Graph['ID_of_from_node']._append(Graph['ID_of_to_node'])).values).astype(int)\n",
    "V_num = V.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(Graph)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Генерация списка смежности\n",
    "Происходит либо импортированием уже ранее сгенерированного списка смежности, либо генерируется и записывается в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export/import generated adjacency list with whitespaces separators\n",
    "# adj_lists = ['adj-bitcoinalpha.csv', 'adj-math.csv', 'adj-chess.csv', 'adj-ucsocial.csv', 'adj-digg.csv']\n",
    "# filename = adj_lists[dataset]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для тестового набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_lists = ['adj-test1.csv', 'adj-test2.csv', 'adj-test3.csv', 'adj-test4.csv', 'adj-test5.csv', 'adj-test6.csv', 'adj-test7.csv', 'adj-middlebury.csv', 'adj-reed98.csv', 'team_10.csv']\n",
    "filename = adj_lists[dataset]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортирование уже существующего списка смежности:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def import_adj():\n",
    "#     matrix = {}\n",
    "#     filepath = \"./adj-lists/\" + filename\n",
    "\n",
    "#     with open(filepath, 'r') as csvfile:\n",
    "#         reader = csv.reader(csvfile, delimiter=' ')\n",
    "        \n",
    "#         for row in reader:\n",
    "#             node = int(row[0])\n",
    "#             adjacent = set(int(row[i]) for i in range(1, len(row)))\n",
    "#             matrix[node] = adjacent\n",
    "\n",
    "#     return matrix\n",
    "\n",
    "# matrix = import_adj()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Генерация списка смежности и его экспорт:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_adj():\n",
    "    filepath = \"./adj-lists/\" + filename\n",
    "\n",
    "    with open(filepath, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=' ')\n",
    "\n",
    "        for node in V:\n",
    "            writer.writerow([node] + ([] if node not in matrix.keys() else list(matrix[node])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate adjacency list for new dataset - new method\n",
    "matrix = {}\n",
    "\n",
    "def add_adjacent(node, adj):\n",
    "    if node not in matrix:\n",
    "        adjacent = set()\n",
    "        adjacent.add(adj)\n",
    "        matrix[node] = adjacent\n",
    "    else:\n",
    "        matrix[node].add(adj)\n",
    "\n",
    "for u, v, w, t in Graph.itertuples(index=False):\n",
    "    if u == v: # to skip loops (in case they're present), because dissartotivity degree formula is 2m/n(n-1) (according to paper)\n",
    "        continue\n",
    "    \n",
    "    add_adjacent(u, v)\n",
    "    add_adjacent(v, u)\n",
    "\n",
    "export_adj()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1: свойства сетей (для статического графа)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1: базовые характеристики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_num = 0\n",
    "for i in V:\n",
    "    if i not in matrix.keys():\n",
    "        continue\n",
    "    \n",
    "    for s in matrix[i]:\n",
    "        if s>i:\n",
    "            E_num+=1\n",
    "\n",
    "density = 2*E_num/(V_num*(V_num-1))\n",
    "print(f'Количество вершин: {V_num};\\nКоличество ребер: {E_num};\\nПлотность: {density}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2\n",
    "Определение компонент слабой свзности и метрик расстояния"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loops_only = set()\n",
    "\n",
    "for v in V:\n",
    "    if v not in matrix.keys():\n",
    "        loops_only.add(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited = set(V).difference(loops_only)\n",
    "Component = set()\n",
    "answer = 0\n",
    "\n",
    "while len(visited):\n",
    "    answer += 1\n",
    "    v = visited.pop()\n",
    "    comp = set([v])\n",
    "    candidates = matrix[v].copy()\n",
    "    while len(candidates):\n",
    "        newCan = set()\n",
    "        for i in candidates:\n",
    "            newCan.update(matrix[i])\n",
    "        comp.update(candidates)\n",
    "        candidates = newCan.difference(comp)\n",
    "    visited -= comp\n",
    "    if len(comp) > len(Component):\n",
    "        Component = comp.copy()\n",
    "\n",
    "answer += len(loops_only)\n",
    "\n",
    "WC_prop = len(Component)/V_num\n",
    "\n",
    "print(f'Количество компонент слабой связности: {answer};\\nРазмер максимальной компоненты: {len(Component)};\\nДоля вершин в максимальной компоненте: {WC_prop}' )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление матрицы расстояний для некоторого подграфа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Path_matrix_calc(subGraph : set):\n",
    "    Path_matrix = {\n",
    "        _from: {\n",
    "             _to: V_num+1 if _from !=_to else 0\n",
    "            for _to in subGraph\n",
    "        }\n",
    "    for _from in subGraph\n",
    "    }\n",
    "\n",
    "    V_calculated = set()\n",
    "    for a in subGraph:\n",
    "        V_calculated.add(a)\n",
    "        V_to_calculate = (subGraph.copy()).difference(V_calculated)\n",
    "        Visited = set()\n",
    "        queue = [a]\n",
    "        dists_queue = {a:0}\n",
    "        while queue and V_to_calculate:\n",
    "            u = queue.pop(0)\n",
    "            dist = dists_queue.pop(u)\n",
    "            Visited.add(u)\n",
    "            u_visit = matrix[u].difference(Visited)\n",
    "            for v in u_visit:\n",
    "                if v in V_to_calculate:\n",
    "                    if dist + 1 < Path_matrix[a][v]:\n",
    "                        Path_matrix[a][v] = dist + 1\n",
    "                        Path_matrix[v][a] = dist + 1\n",
    "                        V_to_calculate.discard(v)\n",
    "                if v not in dists_queue:\n",
    "                    queue.append(v)\n",
    "                    dists_queue[v]=dist+1\n",
    "    return(Path_matrix)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление метрик расстояния.\n",
    "\n",
    "Path_matrix позволяет нам получить информацию об эксцентриситете для каждой вершины.\n",
    "\n",
    "Радиус - минимальное значение эксцентриситета; диаметр - максимальное."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Radius_Diametr_Proc(Path_matrix, subGraph: set):\n",
    "    Exentr = [max(Path_matrix[node].values()) for node in subGraph]\n",
    "    Radius = min(Exentr)\n",
    "    Diametr = max(Exentr)\n",
    "    Distances=[]\n",
    "    visited = set()\n",
    "    for i in subGraph:\n",
    "        visited.add(i)\n",
    "        for j in subGraph:\n",
    "            if j not in visited:\n",
    "                Distances.append(Path_matrix[i][j])\n",
    "    Distances.sort()\n",
    "    # Proc_90 = Distances[int(0.9*len(Distances))]\n",
    "    Proc_90 = np.percentile(Distances, 90)\n",
    "    return(Radius, Diametr, Proc_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_write(name, matrix, subGraph):\n",
    "    with open(f'distance-lists\\{name}', 'w') as f:\n",
    "        f.write(str(subGraph))\n",
    "        f.write('\\n')\n",
    "        f.write(json.dumps(matrix))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание А: оценка расстояний для 500 случайно выбранных вершин из наибольшей компоненты слабой связности (Component)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist_infos_a = ['a-pathmat-bitcoinalpha.txt', 'a-pathmat-math.txt', 'a-pathmat-chess.txt', 'a-pathmat-ucsocial.txt', 'a-pathmat-digg.txt']\n",
    "# dist_infos_b = ['b-pathmat-bitcoinalpha.txt', 'b-pathmat-math.txt', 'b-pathmat-chess.txt', 'b-pathmat-ucsocial.txt', 'b-pathmat-digg.txt']\n",
    "\n",
    "# cur_a = dist_infos_a[dataset]\n",
    "# cur_b = dist_infos_b[dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subGraph_a = set(sample(list(Component), k=500))\n",
    "subGraph_a = Component.copy()\n",
    "Path_matrix_a = Path_matrix_calc(subGraph_a)\n",
    "# txt_write(cur_a, Path_matrix_a, subGraph_a)\n",
    "Radius_a, Diametr_a, Proc_90_a = Radius_Diametr_Proc(Path_matrix_a, subGraph_a)\n",
    "print(f'Радиус: {Radius_a};\\nДиаметр: {Diametr_a};\\n90 процентиль расстояния: {Proc_90_a};')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задание B: оценка расстояний методом \"снежный ком\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subGraph_b = set(sample(list(Component), k=2))\n",
    "subGraph_b = Component.copy()\n",
    "# nodes = subGraph_b.copy()\n",
    "# while len(subGraph_b)<500 and len(nodes)>0:\n",
    "#     u = nodes.pop()\n",
    "#     for i in matrix[u]:\n",
    "#         if len(subGraph_b) == 500:\n",
    "#             break\n",
    "#         nodes.add(i)\n",
    "#         subGraph_b.add(i)\n",
    "Path_matrix_b = Path_matrix_calc(subGraph_b)\n",
    "# txt_write(cur_b, Path_matrix_b, subGraph_b)\n",
    "# Radius_b, Diametr_b, Proc_90_b = (\"---\", \"---\", \"---\")\n",
    "Radius_b, Diametr_b, Proc_90_b = Radius_Diametr_Proc(Path_matrix_b, subGraph_b)\n",
    "print(f'Радиус: {Radius_b};\\nДиаметр: {Diametr_b};\\n90 процентиль расстояния: {Proc_90_b};')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3\n",
    "Вычисление среднего кластерного коэффициента сети для наибольшей компоненты слабой связности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cl(u):\n",
    "    if len(matrix[u])<2:\n",
    "        return 0\n",
    "    neib = matrix[u]\n",
    "    G = len(neib)\n",
    "    _2L=0\n",
    "    for our in neib:\n",
    "        _2L+=len(matrix[our].intersection(neib))\n",
    "    return _2L/(G*(G-1))\n",
    "\n",
    "CL = 0\n",
    "for node in Component:\n",
    "    CL+=Cl(node)\n",
    "\n",
    "Acc = CL/V_num\n",
    "print(f'средний кластерный коэффициент сети: {Acc}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 4\n",
    "Вычисление коэффициента ассортативности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 4\n",
    "V = set(V).difference(loops_only)\n",
    "\n",
    "def R():\n",
    "    r1, r2, r3, re = 0, 0, 0, 0\n",
    "    for node in V:\n",
    "        u = len(matrix[node])\n",
    "        r1+=u\n",
    "        r2+=u*u\n",
    "        r3+=u*u*u\n",
    "        for to in matrix[node]:\n",
    "            re+=u*len(matrix[to])\n",
    "    return (re*r1-r2*r2)/(r3*r1-r2*r2)\n",
    "\n",
    "r = R()\n",
    "print(f'Коэффициент ассортативности: {r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = PrettyTable()\n",
    "stats.field_names = ['Dataset', '|V|', '|E|', 'Density', 'D.a.', 'A.c.c.', 'Diam-A', 'Rad-A', 'Per-A', 'Diam-B', 'Rad-B', 'Per-B', 'WC-num', 'WC-prop']\n",
    "\n",
    "stats.add_row([names[dataset], V_num, E_num, round(density, 4), round(r, 4), round(Acc, 4), Diametr_a, Radius_a, Proc_90_a,  Diametr_b, Radius_b, Proc_90_b, answer, round(WC_prop, 4)])\n",
    "\n",
    "with open('stats\\stats.txt', 'a') as w:\n",
    "    w.write(str(stats) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Часть А\n",
    "Вычисление статических и темпоральных признаков сети (I, II-A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import integrate\n",
    "# from sklearn.model_selection import train_test_split # not using anymore"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка временных отрезков для вычисления темпоральных признаков и решения задачи предсказания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_min = Graph['timestamp'].min()\n",
    "t_max = Graph['timestamp'].max()\n",
    "s = (t_max-t_min) * 0.66 + t_min\n",
    "\n",
    "print(f'Timestamp_min: {t_min}')\n",
    "print(f'Timestamp_max: {t_max}')\n",
    "print(f'Separator: {s}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка к вычислению темпоральных признаков. Поскольку здесь работаем уже с темпоральным графом, то важно учитывать, что между одной парой вершин (u, v) могут существовать несколько рёбер с разными временными метками. С этой целью создаётся словарь словарей: \n",
    "\n",
    "- для внешнего словаря: ключи - все вершины рассматриваемого графа; значения - словарь из смежных с u вершин в промежутке [t_min, s]\n",
    "\n",
    "- для внутреннего словаря: ключ - вершина; значения - множество рёбер, появившихся между (u, v) в промежутке [t_min, s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create adjacency list with timestamps:\n",
    "matrix_t = dict()\n",
    "\n",
    "for u in V:\n",
    "    matrix_t[u] = dict()\n",
    "\n",
    "def add_time (parent, child, timestamp):\n",
    "    if child in matrix_t[parent]:\n",
    "        matrix_t[parent][child].add(timestamp)\n",
    "    else:\n",
    "        timeset = set()\n",
    "        timeset.add(timestamp)\n",
    "        matrix_t[parent][child] = timeset\n",
    "\n",
    "for u, v, w, t in Graph.itertuples(index=False):\n",
    "    # u = _1 # int(row['ID of from node'])\n",
    "    # v = _2 # int(row['ID of to node'])\n",
    "    #timestamp = timestamp#int(row['timestamp'])\n",
    "\n",
    "    if u == v: # skip loops\n",
    "        continue\n",
    "\n",
    "    if t <= s: # work only with data from [t0, s]\n",
    "        add_time(u, v, t)\n",
    "        add_time(v, u, t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовка множества потенциальных пар вершин, которые будут использоваться в задаче бинарной классификации. В соответствии с рекомендациями статьи, в качестве потенциальных пар будем рассматривать только такие пары вершин, что находятся на расстоянии 2 в промежутке [t_min, s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose potential pairs\n",
    "# (u, v) with distance 2 (so they're not connected in [t0, s])\n",
    "potential_pairs = set()\n",
    "\n",
    "for u in V:\n",
    "    u_adj = set(matrix_t[u].keys())\n",
    "    for v in V:\n",
    "        v_adj = set(matrix_t[v].keys())\n",
    "        if u not in matrix_t[v].keys() and u_adj.intersection(v_adj):\n",
    "            if u < v:\n",
    "                potential_pairs.add((u, v))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим множество потенциальных пар на:\n",
    "\n",
    "- positive_pairs: состоит только из пар вершин, которые соединятся в будущем\n",
    "\n",
    "- negative_pairs: состоит только из пар вершин, которые не соединятся в будущем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take all yes/no pairs from potential pairs\n",
    "positive_pairs = set() # all node pairs that will connect\n",
    "negative_pairs = set() # all node pairs that will not connect\n",
    "\n",
    "for u, v in potential_pairs:\n",
    "    if v in matrix[u]:\n",
    "        positive_pairs.add((u, v))\n",
    "\n",
    "negative_pairs = potential_pairs.difference(positive_pairs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выберем 20.000 пар вершин для вычисления векторов-признаков и обучения модели линейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose samples with replacement\n",
    "samples_num = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_samples = choices(list(positive_pairs), k=samples_num)\n",
    "negative_samples = choices(list(negative_pairs), k=samples_num)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление Static Topological Features (I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_vector_s(pair):\n",
    "    parent, child = pair\n",
    "\n",
    "    parent_adj = set(matrix_t[parent].keys())\n",
    "    child_adj = set(matrix_t[child].keys())\n",
    "    commons = parent_adj.intersection(child_adj)\n",
    "\n",
    "    res = [0 for i in range(4)]\n",
    "\n",
    "    res[0] = len(commons) # CN_s\n",
    "    res[2] = len(commons) / len(parent_adj.union(child_adj)) # JC_s\n",
    "    res[3] = len(parent_adj) * len(child_adj) # PA_s\n",
    "\n",
    "    # AA_s\n",
    "    for z in commons:\n",
    "        z_adj = len(set(matrix_t[z].keys()))\n",
    "\n",
    "        res[1] += 1 / np.log10(z_adj)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для пары (1, 2) для тестового набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_test = PrettyTable()\n",
    "static_test.field_names = ['Test name', 'CN_static', 'AA_static', 'JC_static', 'PA_static']\n",
    "\n",
    "pair = (1, 2)\n",
    "pair_test = calculate_vector_s(pair)\n",
    "static_test.add_row([names[dataset], pair_test[0], pair_test[1], pair_test[2], pair_test[3]])\n",
    "\n",
    "with open('stats\\static_tests.txt', 'a') as w:\n",
    "    w.write(str(static_test) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features with past event aggreagtion (II-A)\n",
    "# Step C: weighted topological features for chosen samples only;\n",
    "X_positive_s = [[] for i in range(samples_num)]\n",
    "# labels_positive = []\n",
    "\n",
    "X_negative_s = [[] for i in range(samples_num)]\n",
    "# lables_negative = []\n",
    "\n",
    "for i in range(samples_num):\n",
    "    X_positive_s[i] = calculate_vector_s(positive_samples[i])\n",
    "    X_negative_s[i] = calculate_vector_s(negative_samples[i])\n",
    "\n",
    "shuffle(X_positive_s)\n",
    "shuffle(X_negative_s)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление темпоральных признаков для пары вершин с случае II-A состоит из 3 этапов:\n",
    "\n",
    "1. Temporal weighting\n",
    "\n",
    "2. Past event aggregation\n",
    "\n",
    "3. Computation of weighted topological features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисление весов для рёбер:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step A: temporal weighting\n",
    "l = 0.2 # same value as in paper\n",
    "\n",
    "def weight_linear(times):\n",
    "    weights = set()\n",
    "    for t in times:\n",
    "        T = (t - t_min) / (s - t_min)\n",
    "        weights.add(l + (1 - l) * T)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def weight_exp(times):\n",
    "    weights = set()\n",
    "    for t in times:\n",
    "        T = (t - t_min) / (s - t_min)\n",
    "        weights.add(l + (1 - l) * ((exp(3 * T) - 1) / (exp(3) - 1)))\n",
    "\n",
    "    return weights\n",
    "\n",
    "def weight_square(times):\n",
    "    weights = set()\n",
    "    for t in times:\n",
    "        T = (t - t_min) / (s - t_min)\n",
    "        weights.add(l + (1 - l) * sqrt(T))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У каждой пары вершин есть некоторое множество весов. Далее мы переходим к шагу 2, где агрегируем их для получение одной оценки для пары вершин (u, v) 8-ю разными способами:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features with past event aggreagtion (II-A)\n",
    "# Step B: past event aggregation\n",
    "def aggregate(weights):\n",
    "    # q-quantiles are values that partition a finite set of values into q subsets of (nearly) equal sizes\n",
    "    warr = np.array(list(weights))\n",
    "\n",
    "    zeroth = warr.min() # 0th quantile = minimum\n",
    "    first = warr.max() # 1st quantile = maximum\n",
    "    second = np.median(warr) # 2nd quantile = median\n",
    "    third = np.quantile(warr, 0.3) # 3rd quantile = tertile\n",
    "    fourth = np.quantile(warr, 0.25) # 4th quantile = quartile\n",
    "\n",
    "    sum = np.sum(warr)\n",
    "    mean = np.mean(warr)\n",
    "    variance = np.var(warr)\n",
    "\n",
    "    return [zeroth, first, second, third, fourth, sum, mean, variance]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализация словаря, ключи которого - пары вершин (u, v), а значения - массив из агрегированных оценок шага 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features with past event aggreagtion (II-A)\n",
    "# dict-like structure initialization (aggregated): (node1, node2): [zeroth_linear, ..., variance_linear, zeroth_exp, ..., variance_exp, zeroth_sqrt, ..., variance_sqrt]\n",
    "aggregated = dict()\n",
    "\n",
    "for node in sorted(V):\n",
    "    for adj in matrix_t[node]:\n",
    "        if adj > node:\n",
    "            # convert set of timestamps into set of weights according to formulas\n",
    "            linear = weight_linear(matrix_t[node][adj])\n",
    "            exponent = weight_exp(matrix_t[node][adj])\n",
    "            square = weight_square(matrix_t[node][adj])\n",
    "\n",
    "            res = aggregate(linear)\n",
    "            res += aggregate(exponent)\n",
    "            res += aggregate(square)\n",
    "\n",
    "            aggregated[(node, adj)] = res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация шага 3: вычисление weighted topological features, которые станут векторами-признаками для пар вершин в задаче бинарной классификации:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregated(node, z, cat):\n",
    "    return aggregated[(node, z)][cat] if node < z else aggregated[(z, node)][cat]\n",
    "\n",
    "def calculate_vector(pair):\n",
    "    parent, child = pair\n",
    "\n",
    "    parent_adj = set(matrix_t[parent].keys())\n",
    "    child_adj = set(matrix_t[child].keys())\n",
    "    commons = parent_adj.intersection(child_adj)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for category in range(24):\n",
    "        AA_tmp, CN_tmp, JC_tmp = 0, 0, 0\n",
    "        AA_check, JC_check = False, False\n",
    "\n",
    "        for z in commons:\n",
    "            # common part for AA, CN, JC\n",
    "            num = get_aggregated(parent, z, category)\n",
    "            num += get_aggregated(child, z, category)\n",
    "\n",
    "            # Find AA_tmp\n",
    "            if AA_check == False:\n",
    "                AA_denum = 1\n",
    "\n",
    "                for x in matrix_t[z].keys():\n",
    "                    AA_denum += get_aggregated(z, x, category)\n",
    "\n",
    "                if AA_denum == 1:\n",
    "                    AA_check = True\n",
    "                else:\n",
    "                    AA_tmp += num / np.log10(AA_denum)\n",
    "\n",
    "            # Find CN_tmp\n",
    "            CN_tmp += num\n",
    "\n",
    "            # Find JC_tmp\n",
    "            if JC_check == False:\n",
    "                JC_denum = 0\n",
    "\n",
    "                for x in matrix_t[parent].keys():\n",
    "                    JC_denum += get_aggregated(parent, x, category)\n",
    "                for x in matrix_t[child].keys():\n",
    "                    JC_denum += get_aggregated(child, x, category)\n",
    "                \n",
    "                if JC_denum == 0:\n",
    "                    JC_check = True\n",
    "                else:\n",
    "                    JC_tmp += num / JC_denum\n",
    "\n",
    "        # Find PA_tmp\n",
    "        ares, bres = 0, 0\n",
    "\n",
    "        for a in matrix_t[parent].keys():\n",
    "            ares += get_aggregated(parent, a, category)\n",
    "        for b in matrix_t[child].keys():\n",
    "            bres += get_aggregated(child, b, category)\n",
    "    \n",
    "        PA_tmp = ares * bres\n",
    "\n",
    "        # Results for current category:\n",
    "        res.append(AA_tmp if AA_check == False else 0)\n",
    "        res.append(CN_tmp)\n",
    "        res.append(JC_tmp if JC_check == False else 0)\n",
    "        res.append(PA_tmp)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Часть B"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычисляем векторы-признаки, используя темпоральные признаки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features with past event aggreagtion (II-A)\n",
    "# Step C: weighted topological features for chosen samples only;\n",
    "X_positive_t = [[] for i in range(samples_num)]\n",
    "\n",
    "X_negative_t = [[] for i in range(samples_num)]\n",
    "\n",
    "for i in range(samples_num):\n",
    "    X_positive_t[i] = calculate_vector(positive_samples[i])\n",
    "    X_negative_t[i] = calculate_vector(negative_samples[i])\n",
    "\n",
    "shuffle(X_positive_t)\n",
    "shuffle(X_negative_t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения задачи бинарной классификации, необходимо разделить изначальные 10.000 пар на 2 подмножества: train, на которых обучится алгоритм, и test для оценки результатов обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate calculated feature vectors into 2 groups: train (for learning) and test (for prediction)\n",
    "border = round(samples_num * 0.75)\n",
    "\n",
    "# train/test with static features\n",
    "X_train_s = X_positive_s[:border] + X_negative_s[:border]\n",
    "y_train_s = np.array([1] * border + [0] * border)\n",
    "\n",
    "X_test_s = X_positive_s[border:] + X_negative_s[border:]\n",
    "y_test_s = np.array([1] * (samples_num-border) + [0] * (samples_num-border))\n",
    "\n",
    "# train/test with temporal features\n",
    "X_train_t = X_positive_t[:border] + X_negative_t[:border]\n",
    "y_train_t = np.array([1] * border + [0] * border)\n",
    "\n",
    "X_test_t = X_positive_t[border:] + X_negative_t[border:]\n",
    "y_test_t = np.array([1] * (samples_num-border) + [0] * (samples_num-border))\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make instance of model\n",
    "logisticRegr_s = LogisticRegression(max_iter=1000)\n",
    "logisticRegr_t = LogisticRegression(max_iter=20000)\n",
    "\n",
    "# train model: learning the relationship between feature vectors (X_train) and labels (y_train)\n",
    "logisticRegr_s.fit(X_train_s, y_train_s)\n",
    "logisticRegr_t.fit(X_train_t, y_train_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "# Predict for One Observation (image)\n",
    "answer_prob_s = logisticRegr_s.predict_proba(X_test_s)\n",
    "answer_prob_t = logisticRegr_t.predict_proba(X_test_t)\n",
    "\n",
    "answer_class_s = logisticRegr_s.predict(X_test_s)\n",
    "answer_class_t = logisticRegr_t.predict(X_test_t)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метрики качества и сравнения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_metrics = PrettyTable()\n",
    "basic_metrics.field_names = ['Dataset', 'Features', 'Accuracy', 'Precision', 'Recall']\n",
    "\n",
    "basic_metrics.add_row([names[dataset], 'static', metrics.accuracy_score(answer_class_s, y_test_s),\n",
    "                        metrics.precision_score(answer_class_s, y_test_s), metrics.recall_score(answer_class_s, y_test_s)])\n",
    "basic_metrics.add_row([names[dataset], 'temporal', metrics.accuracy_score(answer_class_t, y_test_t),\n",
    "                        metrics.precision_score(answer_class_t, y_test_t), metrics.recall_score(answer_class_t, y_test_t)])\n",
    "\n",
    "with open('stats\\metrics.txt', 'a') as w:\n",
    "    w.write(str(basic_metrics) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_s = metrics.confusion_matrix(y_test_s, answer_class_s)\n",
    "cm_display_s = metrics.ConfusionMatrixDisplay(confusion_matrix=cm_s)\n",
    "cm_display_s.plot(cmap='cividis')\n",
    "plt.title(f'Confusion Matrix for Static Features ({names[dataset]})')\n",
    "plt.show()\n",
    "\n",
    "cm_t = metrics.confusion_matrix(y_test_t, answer_class_t)\n",
    "cm_display_t = metrics.ConfusionMatrixDisplay(confusion_matrix=cm_t)\n",
    "cm_display_t.plot(cmap='cividis')\n",
    "plt.title(f'Confusion Matrix for Temporal Features ({names[dataset]})')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение ROC-AUC кривой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_s, tpr_s, thresholds_s = metrics.roc_curve(y_test_s, answer_prob_s[:, 1])\n",
    "roc_auc_s = metrics.auc(fpr_s, tpr_s)\n",
    "roc_display_s = metrics.RocCurveDisplay(fpr=fpr_s, tpr=tpr_s, roc_auc=roc_auc_s, estimator_name='example estimator')\n",
    "roc_display_s.plot()\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.title(f'ROC AUC Curve for Static Features ({names[dataset]})')\n",
    "plt.show()\n",
    "\n",
    "fpr_t, tpr_t, thresholds_t = metrics.roc_curve(y_test_t, answer_prob_t[:, 1])\n",
    "roc_auc_t = metrics.auc(fpr_t, tpr_t)\n",
    "roc_display_s = metrics.RocCurveDisplay(fpr=fpr_t, tpr=tpr_t, roc_auc=roc_auc_t, estimator_name='example estimator')\n",
    "roc_display_s.plot()\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.title(f'ROC AUC Curve for Temporal Features ({names[dataset]})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def FRP_TPR(limit, answer_, Ytest):\n",
    "#     FP, TN, TP, FN = 0, 0, 0, 0\n",
    "#     for i in range(len(answer_)):\n",
    "#         if answer_[i]>=limit and Ytest[i]==1:\n",
    "#             TP+=1\n",
    "#         elif answer_[i]>=limit:\n",
    "#             FP+=1\n",
    "#         elif Ytest[i]==1:\n",
    "#             FN+=1\n",
    "#         else:\n",
    "#             TN+=1\n",
    "#     TPR = TP / (TP + FN)\n",
    "#     FPR = FP / (FP + TN)\n",
    "#     return FPR, TPR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# строим график\n",
    "# limit = np.linspace(0, 1, 1000)\n",
    "# fpr, tpr = [], []\n",
    "# for l in limit: \n",
    "#     res = FRP_TPR(l, answer, y_test)\n",
    "#     fpr.append(res[0])\n",
    "#     tpr.append(res[1])\n",
    "# roc_auc = integrate.trapz(tpr[::-1], fpr[::-1])\n",
    "# plt.plot(fpr, tpr, color='darkorange',\n",
    "#          label='ROC кривая (area = %0.2f)' % roc_auc)\n",
    "# plt.plot([0, 1], [0, 1], color='navy', linestyle='--')\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate')\n",
    "# plt.ylabel('True Positive Rate')\n",
    "# plt.title('Пример ROC-кривой')\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
