{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import sample\n",
    "from random import choices\n",
    "from math import exp, sqrt\n",
    "import csv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./datasets/Emails.csv\"\n",
    "headers = [\"ID of from node\",\"ID of to node\", \"weight\", \"timestamp\"]\n",
    "\n",
    "Graph = pd.read_csv(filepath, names=headers, sep=' ').drop_duplicates()\n",
    "V=np.unique((Graph['ID of from node']._append(Graph['ID of to node'])).values).astype(int)\n",
    "V_num = V.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate adjacency list for new dataset - old method\n",
    "# def getValues(i):\n",
    "#     return [ x[0] for x in Graph.loc[(Graph['ID of from node']==i), ['ID of to node']].drop_duplicates().values] + [ x[0] for x in Graph.loc[(Graph['ID of to node']==i), ['ID of from node']].drop_duplicates().values]\n",
    "\n",
    "# matrix = {i: set(getValues(i)) for i in V}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate adjacency list for new dataset - new method\n",
    "matrix = {}\n",
    "\n",
    "def add_adjacent(node, adj):\n",
    "    if node not in matrix:\n",
    "        adjacent = set()\n",
    "        adjacent.add(adj)\n",
    "        matrix[node] = adjacent\n",
    "    else:\n",
    "        matrix[node].add(adj)\n",
    "\n",
    "for index, row in Graph.iterrows():\n",
    "    u = int(row['ID of from node'])\n",
    "    v = int(row['ID of to node'])\n",
    "\n",
    "    if u == v: # to skip loops (in case they're present in dataset), because dissartotivity degree formula is 2m/n(n-1) (according to paper)\n",
    "        continue\n",
    "    \n",
    "    add_adjacent(u, v)\n",
    "    add_adjacent(v, u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print adjacency list (testing purposes)\n",
    "for i in V:\n",
    "    print(matrix[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export/import generated adjacency list with whitespaces separators\n",
    "def export_adj():\n",
    "    filepath = \"./adj-lists/adjacency-list-emails.csv\"\n",
    "\n",
    "    with open(filepath, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter=' ')\n",
    "\n",
    "        for node in V:\n",
    "            writer.writerow([node] + list(matrix[node]))\n",
    "\n",
    "def import_adj():\n",
    "    matrix = {}\n",
    "    filepath = \"./adj-lists/adjacency-list-emails.csv\"\n",
    "\n",
    "    with open(filepath, 'r') as csvfile:\n",
    "        reader = csv.reader(csvfile, delimiter=' ')\n",
    "        \n",
    "        for row in reader:\n",
    "            node = int(row[0])\n",
    "            adjacent = set(int(row[i]) for i in range(1, len(row)))\n",
    "            matrix[node] = adjacent\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call export/import functions\n",
    "# export_adj()\n",
    "matrix = import_adj()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Часть 1. \n",
    "#Задание 1\n",
    "E_num = 0\n",
    "for i in V:\n",
    "    for s in matrix[i]:\n",
    "        if s>i:\n",
    "            E_num+=1\n",
    "print(f'количество вершин: {V_num};\\nколичество ребер: {E_num};\\nплотность: {2*E_num/(V_num*(V_num-1))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited = set(V)\n",
    "Component = set()\n",
    "answer = 0\n",
    "\n",
    "while len(visited):\n",
    "    answer += 1\n",
    "    v = visited.pop()\n",
    "    comp = set([v])\n",
    "    candidates = matrix[v].copy()\n",
    "    while len(candidates):\n",
    "        newCan = set()\n",
    "        for i in candidates:\n",
    "            newCan.update(matrix[i])\n",
    "        comp.update(candidates)\n",
    "        candidates = newCan.difference(comp)\n",
    "    visited -= comp\n",
    "    if len(comp) > len(Component):\n",
    "        Component = comp.copy()\n",
    "\n",
    "\n",
    "\n",
    "print(f'Количество компонент слабой связности: {answer};\\nРазмер максимальной компоненты: {len(Component)};\\nДоля вершин в максимальной компоненте: {len(Component)/V_num}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#задание 2\n",
    "def Path_matrix_calc(subGraph : set):\n",
    "    Path_matrix = {\n",
    "        _from: {\n",
    "             _to: V_num+1 if _from !=_to else 0\n",
    "            for _to in subGraph\n",
    "        }\n",
    "    for _from in subGraph\n",
    "    }\n",
    "\n",
    "    V_calculated = set()\n",
    "    for a in subGraph:\n",
    "        V_calculated.add(a)\n",
    "        V_to_calculate = (subGraph.copy()).difference(V_calculated)\n",
    "        Visited = set()\n",
    "        queue = [a]\n",
    "        dists_queue = {a:0}\n",
    "        while queue and V_to_calculate:\n",
    "            u = queue.pop(0)\n",
    "            dist = dists_queue.pop(u)\n",
    "            Visited.add(u)\n",
    "            u_visit = matrix[u].difference(Visited)\n",
    "            for v in u_visit:\n",
    "                if v in V_to_calculate:\n",
    "                    if dist + 1 < Path_matrix[a][v]:\n",
    "                        Path_matrix[a][v] = dist + 1\n",
    "                        Path_matrix[v][a] = dist + 1\n",
    "                        V_to_calculate.discard(v)\n",
    "                if v not in dists_queue:\n",
    "                    queue.append(v)\n",
    "                    dists_queue[v]=dist+1\n",
    "    return(Path_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Radius_Diametr_Proc(Path_matrix, subGraph: set):\n",
    "    Exentr = [max(Path_matrix[node].values()) for node in subGraph]\n",
    "    Radius = min(Exentr)\n",
    "    Diametr = max(Exentr)\n",
    "    Distances=[]\n",
    "    visited = set()\n",
    "    for i in subGraph:\n",
    "        visited.add(i)\n",
    "        for j in subGraph:\n",
    "            if j not in visited:\n",
    "                Distances.append(Path_matrix[i][j])\n",
    "    Distances.sort()\n",
    "    # Proc_90 = Distances[int(0.9*len(Distances))]\n",
    "    Proc_90 = np.percentile(Distances, 90)\n",
    "    return(Radius, Diametr, Proc_90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def txt_write(name, matrix, subGraph):\n",
    "    with open(f'distance-lists\\{name}', 'w') as f:\n",
    "        f.write(str(subGraph))\n",
    "        f.write('\\n')\n",
    "        f.write(json.dumps(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2а.\n",
    "subGraph_a = set(sample(list(Component), k=500))\n",
    "Path_matrix_a = Path_matrix_calc(subGraph_a)\n",
    "txt_write('Path_matrix_a_prosper_loans.txt', Path_matrix_a, subGraph_a)\n",
    "Radius_a, Diametr_a, Proc_90_a = Radius_Diametr_Proc(Path_matrix_a, subGraph_a)\n",
    "print(f'Радиус: {Radius_a};\\nДиаметр: {Diametr_a};\\n90 процентиль расстояния: {Proc_90_a};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2б.\n",
    "subGraph_b = set(sample(list(Component), k=2))\n",
    "nodes = subGraph_b.copy()\n",
    "while len(subGraph_b)<500 and len(nodes)>0:\n",
    "    u = nodes.pop()\n",
    "    for i in matrix[u]:\n",
    "        if len(subGraph_b) == 500:\n",
    "            break\n",
    "        nodes.add(i)\n",
    "        subGraph_b.add(i)\n",
    "Path_matrix_b = Path_matrix_calc(subGraph_b)\n",
    "txt_write('Path_matrix_b_prosper_loans.txt', Path_matrix_b, subGraph_b)\n",
    "Radius_b, Diametr_b, Proc_90_b = Radius_Diametr_Proc(Path_matrix_b, subGraph_b)\n",
    "print(f'Радиус: {Radius_b};\\nДиаметр: {Diametr_b};\\n90 процентиль расстояния: {Proc_90_b};')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 3\n",
    "def Cl(u):\n",
    "    if len(matrix[u])<2:\n",
    "        return 0\n",
    "    neib = matrix[u]\n",
    "    G = len(neib)\n",
    "    _2L=0\n",
    "    for our in neib:\n",
    "        _2L+=len(matrix[our].intersection(neib))\n",
    "    return _2L/(G*(G-1))\n",
    "\n",
    "CL = 0\n",
    "for node in Component:\n",
    "    CL+=Cl(node)\n",
    "print(f'средний кластерный коэффициент сети: {CL/V_num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Задание 4\n",
    "def R():\n",
    "    r1, r2, r3, re = 0, 0, 0, 0\n",
    "    for node in V:\n",
    "        u = len(matrix[node])\n",
    "        r1+=u\n",
    "        r2+=u*u\n",
    "        r3+=u*u*u\n",
    "        for to in matrix[node]:\n",
    "            re+=u*len(matrix[to])\n",
    "    return (re*r1-r2*r2)/(r3*r1-r2*r2)\n",
    "\n",
    "r = R()\n",
    "print(f'Коэффициент ассортативности: {r}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Часть 2.\n",
    "#статические характеристики\n",
    "CN_static={}\n",
    "AA_static={}\n",
    "JC_static={}\n",
    "PA_static={}\n",
    "visited=set()\n",
    "for adj in V:\n",
    "    visited.add(adj)\n",
    "    for node in matrix[adj]:\n",
    "        if node not in visited:\n",
    "            inter_adj_node = matrix[adj] & matrix[node]\n",
    "            CN_static[(adj, node)] = len(inter_adj_node)\n",
    "            AA_=0\n",
    "            for i in inter_adj_node:\n",
    "                AA_+=(1/np.log10(len(matrix[i])))\n",
    "            AA_static[(adj, node)] = AA_\n",
    "            JC_static[(adj, node)] = len(inter_adj_node)/len(matrix[adj].union(matrix[node]))\n",
    "            PA_static[(adj, node)] = len(matrix[adj])*len(matrix[node])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for weighting step\n",
    "t_min = Graph['timestamp'].min()\n",
    "t_max = Graph['timestamp'].max()\n",
    "s = (t_max-t_min) * 0.66 + t_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'Timestamp_min: {t_min}')\n",
    "# print(f'Timestamp_max: {t_max}')\n",
    "# print(f'Separator: {s}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Часть 2.\n",
    "# Построение векторов признаков для предсказания появления ребер в графе\n",
    "# create adjacency list with timestamps:\n",
    "matrix_t = dict()\n",
    "\n",
    "for u in V:\n",
    "    matrix_t[u] = dict()\n",
    "\n",
    "def add_time (parent, child, timestamp):\n",
    "    if child in matrix_t[parent]:\n",
    "        matrix_t[parent][child].add(timestamp)\n",
    "    else:\n",
    "        timeset = set()\n",
    "        timeset.add(timestamp)\n",
    "        matrix_t[parent][child] = timeset\n",
    "\n",
    "for index, _1, _2, weight, timestamp in Graph.itertuples():\n",
    "    u = _1 # int(row['ID of from node'])\n",
    "    v = _2 # int(row['ID of to node'])\n",
    "    #timestamp = timestamp#int(row['timestamp'])\n",
    "\n",
    "    if u == v: # skip loops\n",
    "        continue\n",
    "\n",
    "    if timestamp <= s: # work only with data from (t0, s)\n",
    "        add_time(u, v, timestamp)\n",
    "        add_time(v, u, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test purposes\n",
    "# print(len(matrix_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test output for temporal adjacency matrix\n",
    "# cnt = 0\n",
    "# for i in matrix_t:\n",
    "#     cnt += len(matrix_t[i].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features with past event aggreagtion (II-A)\n",
    "# Step A: temporal weighting\n",
    "l = 0.2 # same value as in paper\n",
    "\n",
    "def weight_linear(times):\n",
    "    weights = set()\n",
    "    for t in times:\n",
    "        T = (t - t_min) / (s - t_min)\n",
    "        weights.add(l + (1 - l) * T)\n",
    "    \n",
    "    return weights\n",
    "\n",
    "def weight_exp(times):\n",
    "    weights = set()\n",
    "    for t in times:\n",
    "        T = (t - t_min) / (s - t_min)\n",
    "        weights.add(l + (1 - l) * ((exp(3 * T) - 1) / (exp(3) - 1)))\n",
    "\n",
    "    return weights\n",
    "\n",
    "def weight_square(times):\n",
    "    weights = set()\n",
    "    for t in times:\n",
    "        T = (t - t_min) / (s - t_min)\n",
    "        weights.add(l + (1 - l) * sqrt(T))\n",
    "    \n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features with past event aggreagtion (II-A)\n",
    "# Step B: past event aggregation\n",
    "def aggregate(weights):\n",
    "    # q-quantiles are values that partition a finite set of values into q subsets of (nearly) equal sizes\n",
    "    warr = np.array(list(weights))\n",
    "\n",
    "    zeroth = warr.min() # 0th quantile = minimum\n",
    "    first = warr.max() # 1st quantile = maximum\n",
    "    second = np.median(warr) # 2nd quantile = median\n",
    "    third = np.quantile(warr, 0.3) # 3rd quantile = tertile\n",
    "    fourth = np.quantile(warr, 0.25) # 4th quantile = quartile\n",
    "\n",
    "    sum = np.sum(warr)\n",
    "    mean = np.mean(warr)\n",
    "    variance = np.var(warr)\n",
    "\n",
    "    return [zeroth, first, second, third, fourth, sum, mean, variance]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features with past event aggreagtion (II-A)\n",
    "# dict-like structure initialization (aggregated): (node1, node2): [zeroth_linear, ..., variance_linear, zeroth_exp, ..., variance_exp, zeroth_sqrt, ..., variance_sqrt]\n",
    "aggregated = dict()\n",
    "\n",
    "for node in sorted(V):\n",
    "    for adj in matrix_t[node]:\n",
    "        if adj > node:\n",
    "            # convert set of timestamps into set of weights according to formulas\n",
    "            linear = weight_linear(matrix_t[node][adj])\n",
    "            exponent = weight_exp(matrix_t[node][adj])\n",
    "            square = weight_square(matrix_t[node][adj])\n",
    "\n",
    "            res = aggregate(linear)\n",
    "            res += aggregate(exponent)\n",
    "            res += aggregate(square)\n",
    "\n",
    "            aggregated[(node, adj)] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analysis\n",
    "sum = 0\n",
    "min = 168\n",
    "max = 0\n",
    "neighbors = []\n",
    "\n",
    "for node in matrix_t:\n",
    "    length = len(matrix_t[node].keys())\n",
    "    neighbors.append(length)\n",
    "    sum += length\n",
    "    min = length if length < min else min\n",
    "    max = length if length > max else max\n",
    "\n",
    "# neighbors = np.array(neighbors)\n",
    "# for i in range(len(matrix_t)):\n",
    "#     print(neighbors[i])\n",
    "# print(sum / V_num)\n",
    "# print(min)\n",
    "# print(max)\n",
    "# print(st.mode(neighbors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test aggregated\n",
    "# print(len(aggregated[(1, 2)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregated(node, z, cat):\n",
    "    return aggregated[(node, z)][cat] if node < z else aggregated[(z, node)][cat]\n",
    "\n",
    "def calculate_vector(pair):\n",
    "    parent, child = pair\n",
    "\n",
    "    parent_adj = set(matrix_t[parent].keys())\n",
    "    child_adj = set(matrix_t[child].keys())\n",
    "    commons = parent_adj.intersection(child_adj)\n",
    "\n",
    "    res = []\n",
    "\n",
    "    for category in range(24):\n",
    "        AA_tmp, CN_tmp, JC_tmp = 0, 0, 0\n",
    "        AA_check, JC_check = False, False\n",
    "\n",
    "        for z in commons:\n",
    "            # common part for AA, CN, JC\n",
    "            num = get_aggregated(parent, z, category)\n",
    "            num += get_aggregated(child, z, category)\n",
    "\n",
    "            # Find AA_tmp\n",
    "            if AA_check == False:\n",
    "                AA_denum = 1\n",
    "\n",
    "                for x in matrix_t[z].keys():\n",
    "                    AA_denum += get_aggregated(z, x, category)\n",
    "\n",
    "                if AA_denum == 1:\n",
    "                    AA_check = True\n",
    "                else:\n",
    "                    AA_tmp += num / np.log10(AA_denum)\n",
    "\n",
    "            # Find CN_tmp\n",
    "            CN_tmp += num\n",
    "\n",
    "            # Find JC_tmp\n",
    "            if JC_check == False:\n",
    "                JC_denum = 0\n",
    "\n",
    "                for x in matrix_t[parent].keys():\n",
    "                    JC_denum += get_aggregated(parent, x, category)\n",
    "                for x in matrix_t[child].keys():\n",
    "                    JC_denum += get_aggregated(child, x, category)\n",
    "                \n",
    "                if JC_denum == 0:\n",
    "                    JC_check = True\n",
    "                else:\n",
    "                    JC_tmp += num / JC_denum\n",
    "\n",
    "        # Find PA_tmp\n",
    "        ares, bres = 0, 0\n",
    "\n",
    "        for a in matrix_t[parent].keys():\n",
    "            ares += get_aggregated(parent, a, category)\n",
    "        for b in matrix_t[child].keys():\n",
    "            bres += get_aggregated(child, b, category)\n",
    "    \n",
    "        PA_tmp = ares * bres\n",
    "\n",
    "        # Results for current category:\n",
    "        res.append(AA_tmp if AA_check == False else 0)\n",
    "        res.append(CN_tmp)\n",
    "        res.append(JC_tmp if JC_check == False else 0)\n",
    "        res.append(PA_tmp)\n",
    "    \n",
    "    # print(len(res))\n",
    "    return res\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features with past event aggreagtion (II-A) - old method\n",
    "# Step C: weighted topological features\n",
    "\n",
    "# def get_aggregated(node, z, cat):\n",
    "#     return aggregated[(node, z)][cat] if node < z else aggregated[(z, node)][cat]\n",
    "\n",
    "# def AA_tmp(parent, child, commons, category):\n",
    "#     # parent is always smaller than its child, but z - ?\n",
    "#     res = 0\n",
    "\n",
    "#     for z in commons:\n",
    "#         num = get_aggregated(parent, z, category)\n",
    "#         num += get_aggregated(child, z, category)\n",
    "\n",
    "#         denum = 1\n",
    "#         for x in matrix[z]:\n",
    "#             denum += get_aggregated(z, x, category)\n",
    "\n",
    "#         if denum == 1:\n",
    "#             return 0\n",
    "\n",
    "#         res += num / np.log10(denum)\n",
    "\n",
    "#     return res\n",
    "\n",
    "# def CN_tmp(parent, child, commons, category):\n",
    "#     res = 0\n",
    "\n",
    "#     for z in commons:\n",
    "#         res += get_aggregated(parent, z, category)\n",
    "#         res += get_aggregated(child, z, category)\n",
    "\n",
    "#     return res\n",
    "\n",
    "# def JC_tmp(parent, child, commons, category):\n",
    "#     res = 0\n",
    "\n",
    "#     for z in commons:\n",
    "#         num = get_aggregated(parent, z, category)\n",
    "#         num += get_aggregated(child, z, category)\n",
    "\n",
    "#         denum = 0\n",
    "#         for x in matrix[parent]:\n",
    "#             denum += get_aggregated(parent, x, category)\n",
    "#         for x in matrix[child]:\n",
    "#             denum += get_aggregated(child, x, category)\n",
    "        \n",
    "#         if denum == 0:\n",
    "#             return 0\n",
    "        \n",
    "#         res += num / denum\n",
    "    \n",
    "#     return res\n",
    "\n",
    "# def PA_tmp(parent, child, commons, category):\n",
    "#     ares = 0\n",
    "#     bres = 0\n",
    "\n",
    "#     for a in matrix[parent]:\n",
    "#         ares += get_aggregated(parent, a, category)\n",
    "#     for b in matrix[child]:\n",
    "#         bres += get_aggregated(child, b, category)\n",
    "    \n",
    "#     return ares * bres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose potential pairs\n",
    "# (u, v) with distance 2 (so they're not connected in [t0, s])\n",
    "potential_pairs = set()\n",
    "\n",
    "for u in V:\n",
    "    u_adj = set(matrix_t[u].keys())\n",
    "    for v in V:\n",
    "        v_adj = set(matrix_t[v].keys())\n",
    "        if u not in matrix_t[v].keys() and u_adj.intersection(v_adj):\n",
    "            if u < v:\n",
    "                potential_pairs.add((u, v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take all yes/no pairs from potential pairs\n",
    "# y_sample: will connect in (s, t1]\n",
    "# n_sample: won't connect in (s, t1]\n",
    "positive_pairs = set() # all node pairs that will connect\n",
    "negative_pairs = set() # all node pairs that will not connect\n",
    "\n",
    "for u, v in potential_pairs:\n",
    "    if v in matrix[u]:\n",
    "        positive_pairs.add((u, v))\n",
    "\n",
    "# n_pairs = [(pair, 0) for pair in set(potential_pairs).difference(set(y_pairs))]\n",
    "negative_pairs = potential_pairs.difference(positive_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for u, v in y_pairs:\n",
    "#     print(\"Will connect:\", u, v)\n",
    "#     print(len(y_pairs))\n",
    "#     break\n",
    "\n",
    "# for u, v in n_pairs:\n",
    "#     print(\"Will not connect:\", u, v)\n",
    "#     print(len(n_pairs))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad way of initializing sampling of yes\\no\n",
    "stop = 500\n",
    "y_sample = dict()\n",
    "n_sample = dict()\n",
    "\n",
    "for index, _1, _2, weight, timestamp in Graph.itertuples():\n",
    "    u = _1#int(row['ID of from node'])\n",
    "    v = _2#int(row['ID of to node'])\n",
    "    #timestamp = timestamp#int(row['timestamp'])\n",
    "\n",
    "    if u == v: # skip loops\n",
    "        continue\n",
    "\n",
    "    if (u, v) in potential_pairs.keys() and timestamp > s:\n",
    "        if len(y_sample) >= stop:\n",
    "            break\n",
    "\n",
    "        y_sample[(u, v)] = []\n",
    "\n",
    "    if (v, u) in potential_pairs.keys():\n",
    "        if len(y_sample) >= stop:\n",
    "            break\n",
    "\n",
    "        y_sample[(v, u)] = []\n",
    "\n",
    "leftovers = set(potential_pairs.keys()).difference(set(y_sample.keys()))\n",
    "\n",
    "for u, v in leftovers:\n",
    "    if len(n_sample) >= stop:\n",
    "        break\n",
    "\n",
    "    n_sample[(u, v)] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose samples with replacement\n",
    "samples_num = 250\n",
    "\n",
    "positive_samples = choices(list(positive_pairs), k=samples_num)\n",
    "negative_samples = choices(list(negative_pairs), k=samples_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal features with past event aggreagtion (II-A)\n",
    "# Step C: weighted topological features for chosen samples only;\n",
    "X_positive = [[] for i in range(samples_num)]\n",
    "labels_positive = []\n",
    "y_positive = [1] * samples_num\n",
    "\n",
    "X_negative = [[] for i in range(samples_num)]\n",
    "lables_negative = []\n",
    "y_negative = [0] * samples_num\n",
    "\n",
    "for i in range(samples_num):\n",
    "    X_positive[i] = calculate_vector(positive_samples[i])\n",
    "    X_negative[i] = calculate_vector(negative_samples[i])\n",
    "\n",
    "# separate calculated feature vectors into 2 groups: train (for learning) and test (for prediction)\n",
    "border = round(samples_num * 0.75)\n",
    "\n",
    "X_train = X_positive[:border] + X_negative[:border]\n",
    "y_train = np.array(y_positive[:border] + y_negative[:border])\n",
    "\n",
    "X_test = X_positive[border:] + X_negative[border:]\n",
    "y_test = np.array(y_positive[:border] + y_negative[:border])\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(border)\n",
    "# print(len(X_train))\n",
    "# print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make instance of model\n",
    "logisticRegr = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# train model: learning the relationship between feature vectors (X_train) and labels (y_train)\n",
    "logisticRegr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "# Predict for One Observation (image)\n",
    "answer = logisticRegr.predict([X_test[120]])\n",
    "print()\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
